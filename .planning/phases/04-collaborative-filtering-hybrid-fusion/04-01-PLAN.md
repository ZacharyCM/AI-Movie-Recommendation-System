---
phase: 04-collaborative-filtering-hybrid-fusion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/requirements.txt
  - backend/ml/build_collaborative.py
  - backend/ml/download_movielens.py
autonomous: true

must_haves:
  truths:
    - "MovieLens 100K dataset is downloaded and stored locally in backend/ml/data/ml-100k/"
    - "MovieLens movie IDs are mapped to TMDB IDs using links.csv, filtered to only movies in the existing TF-IDF catalog (movie_ids.pkl)"
    - "SVD model is trained on combined MovieLens + real user ratings and saved as svd_model.pkl"
    - "Trainset is saved as cf_trainset.pkl alongside the SVD model"
    - "Running `python -m ml.build_collaborative` produces working .pkl model files"
  artifacts:
    - path: "backend/ml/download_movielens.py"
      provides: "MovieLens 100K download, extraction, and TMDB ID mapping"
      contains: "download_movielens_100k"
    - path: "backend/ml/build_collaborative.py"
      provides: "SVD model training script combining MovieLens + real user ratings"
      contains: "train_svd_model"
    - path: "backend/ml/data/links.csv"
      provides: "MovieLens to TMDB ID mapping file"
    - path: "backend/ml/models/svd_model.pkl"
      provides: "Trained SVD collaborative filtering model"
    - path: "backend/ml/models/cf_trainset.pkl"
      provides: "Surprise trainset for making predictions"
  key_links:
    - from: "backend/ml/build_collaborative.py"
      to: "backend/ml/models/movie_ids.pkl"
      via: "joblib.load to filter MovieLens movies to TF-IDF catalog"
      pattern: "joblib\\.load.*movie_ids"
    - from: "backend/ml/build_collaborative.py"
      to: "backend/ml/download_movielens.py"
      via: "import download and mapping functions"
      pattern: "from.*download_movielens import"
---

<objective>
Train a collaborative filtering model (SVD) on MovieLens 100K seed data mapped to TMDB IDs, producing persisted model artifacts that the hybrid fusion layer (Plan 02) will load at startup.

Purpose: Collaborative filtering needs a trained SVD model to generate predictions. MovieLens seed data ensures CF works from day one without requiring organic user ratings. This plan produces the model files; Plan 02 integrates them into the recommendation pipeline.

Output: `svd_model.pkl` and `cf_trainset.pkl` in `backend/ml/models/`, plus reusable scripts for downloading MovieLens data and training models.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-collaborative-filtering-hybrid-fusion/04-RESEARCH.md
@.planning/phases/03-content-based-recommendations/03-01-SUMMARY.md

Key existing files:
@backend/ml/build_model.py (existing TF-IDF model builder -- follow same patterns)
@backend/ml/models/movie_ids.pkl (list of TMDB IDs in the TF-IDF catalog -- CF movies must be filtered to this)
@backend/requirements.txt (add scikit-surprise, pandas)
@backend/config.py (Supabase settings for fetching real user ratings)
</context>

<tasks>

<task type="auto">
  <name>Task 1: MovieLens download script and TMDB ID mapping</name>
  <files>
    backend/ml/download_movielens.py
    backend/requirements.txt
  </files>
  <action>
    **Add dependencies to requirements.txt:**
    - Add `scikit-surprise==1.1.4` and `pandas>=2.2.0` to backend/requirements.txt

    **Create `backend/ml/download_movielens.py`** with two functions:

    1. `download_movielens_100k(data_dir: str = None) -> str`:
       - Default data_dir to `Path(__file__).parent / "data"`
       - If `data_dir/ml-100k/` already exists, return early (idempotent)
       - Download from `https://files.grouplens.org/datasets/movielens/ml-100k.zip`
       - Extract zip to data_dir (creates ml-100k/ subdirectory)
       - Delete the zip file after extraction
       - Return path to ml-100k/ directory
       - Use `requests` for download (already available via httpx, but use urllib.request from stdlib to avoid adding deps)

    2. `load_movielens_ratings_with_tmdb_mapping(data_dir: str = None) -> pd.DataFrame`:
       - Load ratings from `data_dir/ml-100k/u.data` (tab-separated: user_id, item_id, rating, timestamp)
       - Load MovieLens-to-TMDB mappings from `data_dir/links.csv`
         - This file must be bundled with the project. Create it by downloading from MovieLens 100K's `links.csv` which maps movieId to tmdbId
         - Actually, ML-100K does NOT include links.csv (that's in ML-latest/ML-25M). Instead, use the ML-100K u.item file which has movie titles and years, and create a mapping approach:
           - **Simpler approach**: Download the `ml-latest-small` links.csv from GroupLens which maps MovieLens IDs to TMDB IDs. Save as `data_dir/links.csv`. The `ml-latest-small/links.csv` covers most ML-100K movie IDs.
           - Download URL: `https://files.grouplens.org/datasets/movielens/ml-latest-small.zip` -- extract only `links.csv` from it
       - Merge ratings with links on movieId/item_id to get tmdb_id
       - Load existing TF-IDF movie catalog: `joblib.load("ml/models/movie_ids.pkl")` to get the list of TMDB IDs in the catalog
       - Filter merged ratings to only include movies whose tmdb_id is in the TF-IDF catalog
       - Prefix MovieLens user IDs with "ml_" to prevent collision with real Supabase UUIDs: `"ml_" + str(user_id)`
       - Log: total ratings loaded, ratings after TMDB mapping, ratings after catalog filtering
       - Return DataFrame with columns: `user_id` (str), `movie_id` (int tmdb_id), `rating` (float)

    3. Add a `main()` / `if __name__ == "__main__"` block that calls both functions and prints stats.

    **Important:**
    - Use `urllib.request` for downloads (stdlib, no new deps needed)
    - All paths should be relative to the script's location using `Path(__file__).parent`
    - The `links.csv` download is a one-time operation; cache the file like ml-100k/
    - Handle the case where movie_ids.pkl doesn't exist yet (print warning, return empty DataFrame)
    - MovieLens 100K u.data uses item_id (1-1682), links.csv uses movieId (same IDs for ML-100K era movies)
  </action>
  <verify>
    Run: `cd /Users/zacharym/netflixrecs/backend && pip install scikit-surprise pandas && python -m ml.download_movielens`
    Expected: MovieLens 100K downloaded, links.csv downloaded, ratings mapped to TMDB IDs, filtered to catalog. Should print stats like "Loaded X ratings, mapped Y to TMDB, Z in catalog".
  </verify>
  <done>
    MovieLens 100K is downloaded to `backend/ml/data/ml-100k/`, links.csv exists in `backend/ml/data/`, and `load_movielens_ratings_with_tmdb_mapping()` returns a DataFrame of ratings with TMDB IDs filtered to the existing catalog.
  </done>
</task>

<task type="auto">
  <name>Task 2: SVD model training script with combined MovieLens + real ratings</name>
  <files>
    backend/ml/build_collaborative.py
  </files>
  <action>
    **Create `backend/ml/build_collaborative.py`** that trains an SVD model on combined MovieLens seed data + real user ratings from Supabase.

    Functions:

    1. `get_real_user_ratings() -> pd.DataFrame`:
       - Create Supabase client using settings (supabase_url, supabase_service_role_key)
       - Fetch all rows from "ratings" table: select("user_id, movie_id, rating")
       - If no real ratings or Supabase not configured, return empty DataFrame with columns [user_id, movie_id, rating]
       - Convert user_id to string (UUIDs are already strings from Supabase)
       - Wrap in try/except -- if Supabase fails (not configured, offline), log warning and return empty DataFrame
       - This is expected to be empty in development; MovieLens seed data provides the baseline

    2. `combine_ratings(ml_ratings: pd.DataFrame, real_ratings: pd.DataFrame) -> pd.DataFrame`:
       - Concatenate MovieLens and real ratings
       - Log counts: MovieLens ratings, real ratings, combined total
       - Return combined DataFrame with columns [user_id, movie_id, rating]

    3. `train_svd_model(combined_ratings: pd.DataFrame) -> tuple[SVD, Trainset]`:
       - Create Surprise Dataset from DataFrame using Reader(rating_scale=(1, 5))
       - Dataset.load_from_df(combined_ratings[["user_id", "movie_id", "rating"]], reader)
       - Train SVD with good defaults (skip GridSearchCV for build speed):
         - `SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, verbose=True)`
       - Build full trainset and fit
       - Run a quick sanity check: predict a known MovieLens user-movie pair, print the prediction
       - Return (svd_model, trainset)

    4. `save_models(svd_model, trainset, models_dir: str = None)`:
       - Default models_dir to `Path(__file__).parent / "models"`
       - Save svd_model as `svd_model.pkl` via joblib
       - Save trainset as `cf_trainset.pkl` via joblib
       - Log file sizes

    5. `main()` entry point:
       - Call `download_movielens_100k()` from download_movielens module (ensure data exists)
       - Call `load_movielens_ratings_with_tmdb_mapping()` to get seed ratings
       - Call `get_real_user_ratings()` to get real ratings (may be empty)
       - Call `combine_ratings()` to merge
       - Call `train_svd_model()` on combined
       - Call `save_models()` to persist
       - Print summary with RMSE estimate if possible (optional: run 3-fold cross-validate before full training)

    **Important:**
    - Import from surprise: SVD, Dataset, Reader
    - Import from ml.download_movielens: download_movielens_100k, load_movielens_ratings_with_tmdb_mapping
    - Follow same pattern as build_model.py (asyncio.run in main, but this script can be sync since Surprise is sync)
    - Actually, Supabase client is sync (not async), so the whole script can be synchronous
    - Pin scikit-surprise==1.1.4 in requirements.txt (already done in Task 1)
    - Use `if __name__ == "__main__": main()` pattern, runnable as `python -m ml.build_collaborative`
    - The training should complete in under 2 minutes for MovieLens 100K (100K ratings is fast for SVD)
  </action>
  <verify>
    Run: `cd /Users/zacharym/netflixrecs/backend && python -m ml.build_collaborative`
    Expected: SVD model trained on MovieLens ratings (filtered to TMDB catalog), saved as svd_model.pkl and cf_trainset.pkl. Output shows training progress and final stats.
    Verify files exist: `ls -la backend/ml/models/svd_model.pkl backend/ml/models/cf_trainset.pkl`
  </verify>
  <done>
    Running `python -m ml.build_collaborative` downloads MovieLens (if needed), maps to TMDB IDs, trains SVD, and saves `svd_model.pkl` + `cf_trainset.pkl` to `backend/ml/models/`. Model files exist and are non-empty.
  </done>
</task>

</tasks>

<verification>
1. `backend/ml/data/ml-100k/u.data` exists (MovieLens 100K ratings)
2. `backend/ml/data/links.csv` exists (MovieLens-to-TMDB mapping)
3. `backend/ml/models/svd_model.pkl` exists and is non-empty
4. `backend/ml/models/cf_trainset.pkl` exists and is non-empty
5. `python -c "import joblib; m = joblib.load('ml/models/svd_model.pkl'); print(type(m))"` prints SVD class
6. `pip install scikit-surprise pandas` succeeds
7. No import errors in either script
</verification>

<success_criteria>
- MovieLens 100K downloaded and mapped to TMDB IDs
- SVD model trained and persisted as .pkl files
- Model files loadable with joblib
- Scripts are idempotent (re-running doesn't break anything)
- MovieLens ratings filtered to only movies in TF-IDF catalog (no phantom movie IDs)
</success_criteria>

<output>
After completion, create `.planning/phases/04-collaborative-filtering-hybrid-fusion/04-01-SUMMARY.md`
</output>
