---
phase: 03-content-based-recommendations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/requirements.txt
  - backend/config.py
  - backend/main.py
  - backend/ml/__init__.py
  - backend/ml/build_model.py
  - backend/services/recommender.py
  - backend/routers/recommendations.py
  - backend/schemas/recommendation.py
autonomous: true
user_setup:
  - service: supabase
    why: "Backend needs to query user ratings from Supabase"
    env_vars:
      - name: SUPABASE_URL
        source: "Supabase Dashboard -> Settings -> API -> Project URL (same value as NEXT_PUBLIC_SUPABASE_URL in frontend/.env.local)"
      - name: SUPABASE_SERVICE_ROLE_KEY
        source: "Supabase Dashboard -> Settings -> API -> service_role key (NOT the anon key)"

must_haves:
  truths:
    - "TF-IDF model builds from TMDB popular movies and persists to disk as .pkl files"
    - "Recommendation endpoint returns personalized movies for users with 5+ ratings"
    - "Users with fewer than 5 ratings receive popularity-based fallback results"
    - "Already-rated movies are excluded from recommendation results"
    - "Recommendations are computed on-the-fly from user profile (not cached per-user, so they update after new ratings)"
  artifacts:
    - path: "backend/ml/build_model.py"
      provides: "Script to fetch TMDB popular movies, build metadata soup, fit TF-IDF vectorizer, save .pkl files"
      min_lines: 60
    - path: "backend/services/recommender.py"
      provides: "RecommenderService class with load_model(), build_user_profile(), get_recommendations(), get_popular_fallback()"
      min_lines: 60
    - path: "backend/routers/recommendations.py"
      provides: "GET /api/recommendations endpoint with JWT auth and cold-start fallback"
      exports: ["router"]
    - path: "backend/schemas/recommendation.py"
      provides: "RecommendationResponse and RecommendationListResponse Pydantic models"
      contains: "class RecommendationResponse"
    - path: "backend/ml/__init__.py"
      provides: "Package marker for ml module"
  key_links:
    - from: "backend/ml/build_model.py"
      to: "TMDB API"
      via: "httpx requests to /movie/popular and /movie/{id} with keywords append"
      pattern: "tmdb.*popular|tmdb.*movie.*keywords"
    - from: "backend/services/recommender.py"
      to: "backend/ml/models/*.pkl"
      via: "joblib.load at startup"
      pattern: "joblib\\.load"
    - from: "backend/routers/recommendations.py"
      to: "backend/services/recommender.py"
      via: "RecommenderService method calls"
      pattern: "recommender\\.(get_recommendations|get_popular_fallback)"
    - from: "backend/main.py"
      to: "backend/services/recommender.py"
      via: "FastAPI lifespan loads model at startup"
      pattern: "lifespan|recommender.*load"
---

<objective>
Build the content-based recommendation engine backend: a TF-IDF model builder script, a recommender service, and a FastAPI endpoint that returns personalized movie recommendations based on user ratings.

Purpose: This is the core ML backend for Phase 3. It transforms user ratings into a taste profile vector and computes cosine similarity against a pre-built TF-IDF matrix of movie metadata to generate personalized recommendations.

Output: Working `/api/recommendations` endpoint that returns top-N movie recommendations (or popularity fallback for cold-start users), plus a `build_model.py` script to create the TF-IDF model from TMDB data.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-content-based-recommendations/03-RESEARCH.md
@backend/main.py
@backend/config.py
@backend/requirements.txt
@backend/routers/movies.py
@backend/schemas/movie.py
@backend/services/tmdb.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: TF-IDF Model Builder and Recommender Service</name>
  <files>
    backend/requirements.txt
    backend/config.py
    backend/ml/__init__.py
    backend/ml/build_model.py
    backend/services/recommender.py
    backend/schemas/recommendation.py
  </files>
  <action>
**1. Update requirements.txt** -- Add these dependencies:
```
scikit-learn==1.6.1
numpy>=1.26.0
joblib>=1.4.0
supabase>=2.0.0
```
Note: Use scikit-learn 1.6.1 (latest stable as of Feb 2026, not 1.8.0 which doesn't exist yet). Pin exact version for model compatibility per research pitfall #1.

**2. Update config.py** -- Add Supabase settings to the Settings class:
```python
supabase_url: str = ""
supabase_service_role_key: str = ""
```
These load from SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY env vars via pydantic-settings.

**3. Create backend/ml/__init__.py** -- Empty file (package marker).

**4. Create backend/ml/build_model.py** -- Standalone script to build TF-IDF model:
- Fetch top 250 popular movies from TMDB (pages 1-13, ~20 movies per page). Start with 250 for development; this is sufficient for demo purposes and keeps build time fast.
- For each movie, fetch detailed data including keywords via `/movie/{id}?append_to_response=keywords,credits`
- Build metadata "soup" for each movie: concatenate genres (names), keywords (names), overview text, top 3 cast names (spaces removed, e.g., "BradPitt"), director name (spaces removed)
- Fit TfidfVectorizer with params: `max_features=5000, stop_words='english', ngram_range=(1,2), min_df=2, max_df=0.8, norm='l2'`
- Note: With a 250-movie corpus, `min_df=2` may filter aggressively. If the fitted vocabulary is very small (< 100 features), fall back to `min_df=1`.
- Save three files with joblib to `backend/ml/models/`:
  - `tfidf_vectorizer.pkl` (fitted vectorizer)
  - `tfidf_matrix.pkl` (sparse CSR matrix)
  - `movie_ids.pkl` (list of TMDB movie IDs aligned with matrix rows)
- Use httpx with rate limiting (0.25s delay between requests) to respect TMDB API limits
- Print progress: "Fetching page X...", "Processing movie: Title", "Model saved. Matrix shape: (N, M)"
- Script should be runnable as: `cd backend && python -m ml.build_model`
- Load TMDB API key from config (import settings from config.py)

**5. Create backend/schemas/recommendation.py** -- Pydantic response models:
```python
class RecommendationResponse(BaseModel):
    movie_id: int
    title: str
    poster_path: str | None
    overview: str
    vote_average: float
    release_date: str
    score: float          # cosine similarity score (0-1)
    reason: str           # "content_based" or "popular"

class RecommendationListResponse(BaseModel):
    recommendations: list[RecommendationResponse]
    strategy: str         # "content_based" or "popularity_fallback"
    total_ratings: int    # how many ratings user has
```

**6. Create backend/services/recommender.py** -- RecommenderService class:
- `__init__()`: Initialize with empty state (vectorizer, tfidf_matrix, movie_ids, movie_id_to_index all None)
- `load_model(model_dir: str)`: Load all three .pkl files with joblib, build movie_id_to_index dict. Log "Recommender model loaded: {shape}" or warn if files not found (don't crash -- allow server to start without model for development)
- `is_loaded() -> bool`: Return whether model is loaded
- `build_user_profile(ratings: list[dict]) -> sparse matrix or None`: Takes list of {movie_id, rating} dicts. Filter for ratings >= 4 (high ratings only). Map movie_ids to matrix indices (skip any not in corpus). Compute weighted average of TF-IDF vectors. Return None if no valid high ratings found. Keep sparse throughout (never call .toarray()).
- `get_recommendations(ratings: list[dict], top_n: int = 10) -> list[dict]`: Build user profile, compute cosine_similarity against full matrix, sort descending, exclude already-rated movie_ids, return top_n as list of {movie_id, score} dicts. If user_profile is None (no high ratings), return empty list.
- `get_popular_fallback(top_n: int = 10) -> list[int]`: Return the first top_n movie_ids from the corpus (these are popular movies from TMDB, already sorted by popularity).

IMPORTANT: Do NOT cache user profiles. Rebuild on every request. The TF-IDF matrix is pre-computed and cached, but user profiles must be fresh so recommendations update immediately after new ratings.
  </action>
  <verify>
- `cd /Users/zacharym/netflixrecs/backend && pip install -r requirements.txt` succeeds
- `python -c "from services.recommender import RecommenderService; r = RecommenderService(); print('OK')"` succeeds
- `python -c "from schemas.recommendation import RecommendationResponse; print('OK')"` succeeds
- `python -c "from ml.build_model import main; print('build_model importable')"` or similar import check succeeds
  </verify>
  <done>
- requirements.txt includes scikit-learn, numpy, joblib, supabase
- config.py has supabase_url and supabase_service_role_key fields
- build_model.py can fetch TMDB data, create metadata soup, fit TF-IDF, and save .pkl files
- RecommenderService loads .pkl files, builds user profiles from ratings, returns ranked recommendations excluding rated movies
- Pydantic schemas define RecommendationResponse with movie_id, title, score, reason fields
  </done>
</task>

<task type="auto">
  <name>Task 2: FastAPI Recommendation Endpoint with Lifespan Integration</name>
  <files>
    backend/main.py
    backend/routers/recommendations.py
  </files>
  <action>
**1. Update backend/main.py** -- Add FastAPI lifespan for model loading:
- Import `asynccontextmanager` from contextlib
- Import `RecommenderService` from services.recommender
- Create a module-level `recommender_service = RecommenderService()` instance
- Create lifespan async context manager that calls `recommender_service.load_model("ml/models")` on startup, yields, then does nothing on shutdown (model cleanup handled by GC)
- Pass `lifespan=lifespan` to FastAPI() constructor
- Import and include the recommendations router: `from routers.recommendations import router as recommendations_router`
- `app.include_router(recommendations_router)`
- Export `recommender_service` so the router can import it

**2. Create backend/routers/recommendations.py** -- API endpoint:
- Create APIRouter with `prefix="/api/recommendations"` and `tags=["recommendations"]`
- Create helper function `get_current_user_id(authorization: str = Header(None)) -> str`:
  - Extract JWT from "Bearer {token}" header
  - Decode JWT using PyJWT (already included with supabase package) to extract user_id from `sub` claim
  - Use the Supabase JWT secret (can be derived from SUPABASE_SERVICE_ROLE_KEY or use a simpler approach)
  - SIMPLER APPROACH: Use supabase-py admin client to verify the token: create Supabase client with service role key, call `auth.get_user(token)` to validate and get user_id. This avoids manually verifying JWTs.
  - Raise HTTPException(401) if no token or invalid token
- Create `GET /` endpoint (maps to `GET /api/recommendations`):
  - Accept `top_n: int = Query(10, ge=1, le=50)` parameter
  - Get user_id from `get_current_user_id` dependency
  - Check if recommender model is loaded; if not, raise HTTPException(503, "Recommendation model not available")
  - Create Supabase admin client using service role key from settings
  - Query ratings table: `supabase.table("ratings").select("movie_id, rating").eq("user_id", user_id).execute()`
  - If len(ratings) < 5: use popularity fallback strategy
    - Get popular movie_ids from recommender_service.get_popular_fallback(top_n)
    - For each movie_id, fetch movie details from TMDB (reuse TMDBService) to get title, poster_path, overview, etc.
    - Return RecommendationListResponse with strategy="popularity_fallback"
  - If len(ratings) >= 5: use content-based strategy
    - Call recommender_service.get_recommendations(ratings, top_n)
    - For each recommended movie_id, fetch movie details from TMDB
    - Return RecommendationListResponse with strategy="content_based"
  - Include total_ratings count in response

Note on TMDB fetching: For each recommendation, we need movie details (title, poster, overview). Use TMDBService.get_movie_details() but only extract the fields needed. To avoid excessive TMDB API calls, fetch details concurrently using asyncio.gather() with a reasonable batch size. Consider that TMDB rate limit is ~40 req/10s, and we're fetching at most 50 movies.
  </action>
  <verify>
- `cd /Users/zacharym/netflixrecs/backend && python -c "from main import app; print('App starts OK')"` succeeds
- `cd /Users/zacharym/netflixrecs/backend && uvicorn main:app --host 0.0.0.0 --port 8000` starts without errors (model files may not exist yet, but server should start gracefully)
- Swagger docs at http://localhost:8000/docs show `/api/recommendations` endpoint
- GET /api/recommendations without auth returns 401
  </verify>
  <done>
- main.py uses lifespan to load recommender model at startup
- GET /api/recommendations requires Bearer token auth, returns personalized recommendations for users with 5+ ratings
- Users with < 5 ratings receive popularity_fallback strategy with popular TMDB movies
- Response includes strategy field ("content_based" or "popularity_fallback") and total_ratings count
- Server starts gracefully even if .pkl model files don't exist (logs warning, returns 503 for recommendations)
  </done>
</task>

</tasks>

<verification>
1. Run `python -m ml.build_model` from backend/ directory -- should fetch TMDB data, build TF-IDF model, and save .pkl files to ml/models/
2. Start server with `uvicorn main:app` -- should log "Recommender model loaded" at startup
3. GET /api/recommendations without auth header returns 401
4. GET /api/recommendations with valid Bearer token returns recommendations (content_based if 5+ ratings, popularity_fallback otherwise)
5. Each recommendation includes movie_id, title, poster_path, overview, vote_average, score, reason
6. Already-rated movies do not appear in content_based recommendations
</verification>

<success_criteria>
- TF-IDF model builds and persists successfully from TMDB data
- Recommendation endpoint returns personalized results reflecting user's taste profile
- Cold-start users (< 5 ratings) receive popularity-based fallback
- Server starts gracefully with or without model files
- No user profile caching -- recommendations refresh on every request
</success_criteria>

<output>
After completion, create `.planning/phases/03-content-based-recommendations/03-01-SUMMARY.md`
</output>
